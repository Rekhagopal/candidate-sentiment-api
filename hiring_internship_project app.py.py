# -*- coding: utf-8 -*-
"""Hiring  internship project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ARWrkqiK15I8ItItXeMYEPcHXtpfQG5C
"""

!pip install yt-dlp

!yt-dlp -o "interview_videos/%(title)s.%(ext)s" -f bestvideo+bestaudio --merge-output-format mp4 "https://youtu.be/LuU9fm20JPU?feature=shared"

!yt-dlp -o "interview_videos/%(title)s.%(ext)s" -f bestvideo+bestaudio --merge-output-format mp4 "https://youtu.be/2sWVLMVQsu0?feature=shared"

!yt-dlp -o "interview_videos/%(title)s.%(ext)s" -f bestvideo+bestaudio --merge-output-format mp4 "https://youtu.be/4mG7morAasw?feature=shared"

!yt-dlp -o "interview_videos/%(title)s.%(ext)s" -f bestvideo+bestaudio --merge-output-format mp4 "https://youtu.be/2sWVLMVQsu0?feature=shared"

from google.colab import files
files.download("interview_videos/A⧸B Testing Interview with a Google Data Scientist.mp4")

!yt-dlp -o "interview_videos/%(title)s.%(ext)s" -f bestvideo+bestaudio --merge-output-format mp4 "https://youtu.be/4mG7morAasw?feature=shared"

from google.colab import files
files.download("interview_videos/LinkedIn Machine Learning Mock Interview - Design a recommendation engine.mp4")



!yt-dlp -o "interview_videos/%(title)s.%(ext)s" -f bestvideo+bestaudio --merge-output-format mp4 "https://youtu.be/dPSzIiW1x8s?feature=shared"

from google.colab import files
files.download("interview_videos/LinkedIn Data Scientist Mock Interview + Feedback with Ex-DoorDash & Spotify Data Scientist!.mp4")

!yt-dlp -o "interview_videos/%(title)s.%(ext)s" -f bestvideo+bestaudio --merge-output-format mp4 "https://youtu.be/VpTlNRUcIDo?feature=shared"

from google.colab import files
files.download("interview_videos/A⧸B Testing Fundamentals： What Every Data Scientist Needs to Know!.mp4")

!pip install moviepy

from moviepy.editor import VideoFileClip

# Define the input video file and output audio file
mp4_file = "interview_videos/A⧸B Testing Fundamentals： What Every Data Scientist Needs to Know!.mp4"
mp3_file = "audio 1.mp3"

# Load the video clip
video_clip = VideoFileClip(mp4_file)

# Extract the audio from the video clip
audio_clip = video_clip.audio

# Write the audio to a separate file
audio_clip.write_audiofile('audio 1.mp3')

# Close the video and audio clips
audio_clip.close()
video_clip.close()

print("Audio extraction successful!")

from moviepy.editor import VideoFileClip

# Define the input video file and output audio file
mp4_file = "/content/interview_videos/Complex Datasets with SQL (Data Scientist Mock Interview).mp4"
mp3_file = "audio 2.mp3"

# Load the video clip
video_clip = VideoFileClip(mp4_file)

# Extract the audio from the video clip
audio_clip = video_clip.audio

# Write the audio to a separate file
audio_clip.write_audiofile('audio 2.mp3')

# Close the video and audio clips
audio_clip.close()
video_clip.close()

print("Audio extraction successful!")

from moviepy.editor import VideoFileClip

# Define the input video file and output audio file
mp4_file = "/content/interview_videos/LinkedIn Data Scientist Mock Interview + Feedback with Ex-DoorDash & Spotify Data Scientist!.mp4"
mp3_file = "audio 3.mp3"

# Load the video clip
video_clip = VideoFileClip(mp4_file)

# Extract the audio from the video clip
audio_clip = video_clip.audio

# Write the audio to a separate file
audio_clip.write_audiofile('audio 3.mp3')

# Close the video and audio clips
audio_clip.close()
video_clip.close()

print("Audio extraction successful!")

from moviepy.editor import VideoFileClip

# Define the input video file and output audio file
mp4_file = "/content/interview_videos/LinkedIn Machine Learning Mock Interview - Design a recommendation engine.mp4"
mp3_file = "audio 4.mp3"

# Load the video clip
video_clip = VideoFileClip(mp4_file)

# Extract the audio from the video clip
audio_clip = video_clip.audio

# Write the audio to a separate file
audio_clip.write_audiofile('audio 4.mp3')

# Close the video and audio clips
audio_clip.close()
video_clip.close()

print("Audio extraction successful!")

!pip install openai-whisper

import whisper

# Load Whisper model
model = whisper.load_model("base")

# Transcribe audio
result = model.transcribe("audio 1.mp3")

# Get segments from the result
segments = result['segments']

# Alternating speaker labeling
speakers = ['Interviewer', 'Candidate']
speaker_index = 0

# Build transcript with speaker tags
transcript_lines = []
for segment in segments:
    speaker = speakers[speaker_index % 2]
    line = f"{speaker}: {segment['text'].strip()}"
    transcript_lines.append(line)
    speaker_index += 1

# Join lines into full transcript
final_transcript = "\n\n".join(transcript_lines)

# Print or save
print("\n--- Transcript with Speaker Labels ---\n")
print(final_transcript)

with open("speaker_labeled_transcript.txt", "w", encoding="utf-8") as file:
    file.write(final_transcript)

print("\n✅ Transcript saved as 'speaker_labeled_transcript.txt'")

import whisper

# Load Whisper model
model = whisper.load_model("base")

# Transcribe audio
result = model.transcribe("audio 2.mp3")

# Get segments from the result
segments = result['segments']

# Alternating speaker labeling
speakers = ['Interviewer', 'Candidate']
speaker_index = 0

# Build transcript with speaker tags
transcript_lines = []
for segment in segments:
    speaker = speakers[speaker_index % 2]
    line = f"{speaker}: {segment['text'].strip()}"
    transcript_lines.append(line)
    speaker_index += 1

# Join lines into full transcript
final_transcript2 = "\n\n".join(transcript_lines)

# Print or save
print("\n--- Transcript with Speaker Labels ---\n")
print(final_transcript2)

with open("speaker_labeled_transcript2.txt", "w", encoding="utf-8") as file:
    file.write(final_transcript2)

print("\n✅ Transcript saved as 'speaker_labeled_transcript2.txt'")



import whisper

# Load Whisper model
model = whisper.load_model("base")

# Transcribe audio
result = model.transcribe("audio 3.mp3")

# Get segments from the result
segments = result['segments']

# Alternating speaker labeling
speakers = ['Interviewer', 'Candidate']
speaker_index = 0

# Build transcript with speaker tags
transcript_lines = []
for segment in segments:
    speaker = speakers[speaker_index % 2]
    line = f"{speaker}: {segment['text'].strip()}"
    transcript_lines.append(line)
    speaker_index += 1

# Join lines into full transcript
final_transcript3 = "\n\n".join(transcript_lines)

# Print or save
print("\n--- Transcript with Speaker Labels ---\n")
print(final_transcript3)

with open("speaker_labeled_transcript3.txt", "w", encoding="utf-8") as file:
    file.write(final_transcript3)

print("\n✅ Transcript saved as 'speaker_labeled_transcript3.txt'")

!pip install pandas openpyxl nltk matplotlib seaborn

import pandas as pd
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

# Download lexicon once
nltk.download("vader_lexicon")

# Initialize VADER
sia = SentimentIntensityAnalyzer()

# Read the transcript
with open("speaker_labeled_transcript.txt", "r", encoding="utf-8") as f:
    lines = f.readlines()

# Split into speaker and text
data = []
for line in lines:
    if ":" in line:
        speaker, text = line.split(":", 1)
        data.append({"Speaker": speaker.strip(), "Text": text.strip()})

# Create DataFrame
df_text = pd.DataFrame(data)

# Function to get compound score
def get_sentiment(text):
    return sia.polarity_scores(text)['compound']

# Apply sentiment analysis
df_text["SentimentScore"] = df_text["Text"].apply(get_sentiment)

# Optional: Add a sentiment label
df_text["SentimentLabel"] = df_text["SentimentScore"].apply(
    lambda score: "Positive" if score > 0.05 else ("Negative" if score < -0.05 else "Neutral")
)

df_text.head()



# Read the transcript
with open("speaker_labeled_transcript2.txt", "r", encoding="utf-8") as f:
    lines = f.readlines()

# Split into speaker and text
data = []
for line in lines:
    if ":" in line:
        speaker, text = line.split(":", 1)
        data.append({"Speaker": speaker.strip(), "Text": text.strip()})

# Create DataFrame
df_text = pd.DataFrame(data)



# Function to get compound score
def get_sentiment(text):
    return sia.polarity_scores(text)['compound']

# Apply sentiment analysis
df_text["SentimentScore"] = df_text["Text"].apply(get_sentiment)

# Optional: Add a sentiment label
df_text["SentimentLabel"] = df_text["SentimentScore"].apply(
    lambda score: "Positive" if score > 0.05 else ("Negative" if score < -0.05 else "Neutral")
)

df_text.head()



# Read the transcript
with open("speaker_labeled_transcript3.txt", "r", encoding="utf-8") as f:
    lines = f.readlines()

# Split into speaker and text
data = []
for line in lines:
    if ":" in line:
        speaker, text = line.split(":", 1)
        data.append({"Speaker": speaker.strip(), "Text": text.strip()})

# Create DataFrame
df_text = pd.DataFrame(data)

# Function to get compound score
def get_sentiment(text):
    return sia.polarity_scores(text)['compound']

# Apply sentiment analysis
df_text["SentimentScore"] = df_text["Text"].apply(get_sentiment)

# Optional: Add a sentiment label
df_text["SentimentLabel"] = df_text["SentimentScore"].apply(
    lambda score: "Positive" if score > 0.05 else ("Negative" if score < -0.05 else "Neutral")
)

df_text.head()

import pandas as pd
from nltk.sentiment import SentimentIntensityAnalyzer
import nltk
nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()

# File list
files = {
    "Candidate 1": "speaker_labeled_transcript.txt",
    "Candidate 2": "speaker_labeled_transcript2.txt",
    "Candidate 3": "speaker_labeled_transcript3.txt"
}

results = []

for candidate_name, filepath in files.items():
    # Read file
    with open(filepath, "r", encoding="utf-8") as f:
        lines = f.readlines()

    # Extract speaker/text
    data = []
    for line in lines:
        if ":" in line:
            speaker, text = line.split(":", 1)
            data.append({"Speaker": speaker.strip(), "Text": text.strip()})

    df = pd.DataFrame(data)

    # Sentiment Score
    df["SentimentScore"] = df["Text"].apply(lambda x: sia.polarity_scores(x)["compound"])

    # Filter only Candidate's lines
    df_candidate = df[df["Speaker"].str.lower().str.contains("candidate")]

    # Average sentiment of Candidate
    avg_score = round(df_candidate["SentimentScore"].mean(), 4) if not df_candidate.empty else 0.0

    results.append({
        "Candidate": candidate_name,
        "AvgSentiment": avg_score
    })

# Convert to DataFrame
result_df = pd.DataFrame(results).sort_values(by="AvgSentiment", ascending=False)

# Show results
print("📊 Sentiment Score Summary for All Candidates:\n")
print(result_df)

# Show most likely to accept offer
top = result_df.iloc[0]
print(f"\n🏆 {top['Candidate']} is most likely to accept the offer (Avg Sentiment Score: {top['AvgSentiment']})")

import matplotlib.pyplot as plt

plt.figure(figsize=(7, 4))
plt.bar(result_df["Candidate"], result_df["AvgSentiment"], color='skyblue')
plt.title("Average Sentiment Score by Candidate")
plt.ylabel("Avg Compound Sentiment Score")
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--')
plt.show()

from flask import Flask, request, jsonify
import whisper
import tempfile
import os
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from moviepy.editor import VideoFileClip

nltk.download("vader_lexicon")

app = Flask(__name__)
model = whisper.load_model("base")
sia = SentimentIntensityAnalyzer()

# ✅ Extract audio using moviepy
def extract_audio_with_moviepy(video_path, audio_path):
    clip = VideoFileClip(video_path)
    clip.audio.write_audiofile(audio_path, fps=16000, codec='pcm_s16le')  # wav format

def transcribe_audio(audio_path):
    result = model.transcribe(audio_path)
    return result['text']

def analyze_sentiment(transcript):
    lines = transcript.split(".")
    scores = [sia.polarity_scores(line.strip())['compound'] for line in lines if line.strip()]
    return round(sum(scores) / len(scores), 4) if scores else 0.0

@app.route("/predict", methods=["POST"])
def predict():
    files = request.files.getlist("videos")
    results = []

    for i, file in enumerate(files):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video:
            file.save(temp_video.name)
            audio_path = temp_video.name.replace(".mp4", ".wav")

            # 🎬 Use moviepy to extract audio
            extract_audio_with_moviepy(temp_video.name, audio_path)

            transcript = transcribe_audio(audio_path)
            score = analyze_sentiment(transcript)

            results.append({
                "Candidate": f"Candidate {i+1}",
                "AverageSentimentScore": score
            })

            os.remove(temp_video.name)
            os.remove(audio_path)

    results = sorted(results, key=lambda x: x["AverageSentimentScore"], reverse=True)
    return jsonify(results)

if __name__ == "__main__":
    app.run(debug=True)